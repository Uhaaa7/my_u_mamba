import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager
from torch import nn
from nnunetv2.nets.UMambaBot_3d import get_umamba_bot_3d_from_plans
from nnunetv2.nets.UMambaBot_2d import get_umamba_bot_2d_from_plans

class nnUNetTrainerUMambaBaseline(nnUNetTrainer):
    """
    ã€Baseline å®Œç¾å¯¹ç…§ç»„ã€‘
    ä¸¥æ ¼æ§åˆ¶æ‰€æœ‰å˜é‡ä¸ Ours (SDG) ä¸€è‡´ï¼š
    1. LR = 1e-3
    2. Gradient Clipping = 12.0
    3. Patch Size å¿…é¡»åœ¨ plans.json æˆ– è¿è¡Œæ—¶ä¿è¯æ˜¯ [384, 384]
    4. å”¯ä¸€å˜é‡ï¼šSDG å…³é—­ (enable_sdg=False)
    """

    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, unpack_dataset: bool = True,
                 device: torch.device = torch.device('cuda')):
        super().__init__(plans, configuration, fold, dataset_json, unpack_dataset, device)
        
        # âœ… 1. å¼ºåˆ¶å¯¹é½å­¦ä¹ ç‡
        self.initial_lr = 1e-3
        self.num_epochs = 500
        print("ğŸ¢ [Baseline] Initial LR set to 1e-3 (Matched with Ours)")

    @staticmethod
    def build_network_architecture(plans_manager: PlansManager,
                                   dataset_json,
                                   configuration_manager: ConfigurationManager,
                                   num_input_channels,
                                   enable_deep_supervision: bool = True) -> nn.Module:

        # âš ï¸ æ£€æŸ¥ Patch Size æ˜¯å¦ä¸€è‡´
        # å¦‚æœä½ çš„ plans.json è¿˜æ²¡æ”¹å›æ¥ï¼Œè¿™é‡Œåº”è¯¥ä¼šè‡ªåŠ¨è¯»åˆ° [384, 384]
        # å¦‚æœè¿™é‡Œè¯»åˆ°çš„æ˜¯ [512, 512]ï¼Œä½ éœ€è¦åœ¨ plans æ–‡ä»¶é‡Œæ”¹ï¼Œæˆ–è€…åœ¨è¿™é‡Œå¼ºåˆ¶æŠ¥é”™æé†’è‡ªå·±
        current_ps = configuration_manager.patch_size
        print(f"ğŸ¢ [Baseline] Current Patch Size: {current_ps}")
        
        if len(configuration_manager.patch_size) == 2:
            model = get_umamba_bot_2d_from_plans(
                plans_manager, 
                dataset_json, 
                configuration_manager,
                num_input_channels, 
                deep_supervision=enable_deep_supervision,
                # âœ… 2. å”¯ä¸€çš„ä¸åŒï¼šå…³é—­ SDG
                enable_sdg=False 
            )
        elif len(configuration_manager.patch_size) == 3:
            model = get_umamba_bot_3d_from_plans(
                plans_manager, 
                dataset_json, 
                configuration_manager,
                num_input_channels, 
                deep_supervision=enable_deep_supervision
            )
        else:
            raise NotImplementedError("Only 2D and 3D models are supported")
        
        print("ğŸ¢ [Baseline] SDG Module: DISABLED âŒ")
        return model

    # âœ… 3. å¼ºåˆ¶å¯¹é½æ¢¯åº¦è£å‰ª
    def train_step(self, batch: dict) -> dict:
        data = batch['data']
        target = batch['target']

        data = data.to(self.device, non_blocking=True)
        if isinstance(target, list):
            target = [i.to(self.device, non_blocking=True) for i in target]
        else:
            target = target.to(self.device, non_blocking=True)

        self.optimizer.zero_grad(set_to_none=True)

        # ä¿æŒ FP16 (Autocast)
        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):
            output = self.network(data)
            l = self.loss(output, target)

        if self.grad_scaler is not None:
            self.grad_scaler.scale(l).backward()
            
            # ğŸ”¥ å¿…é¡»åŠ ï¼ä¸ºäº†å…¬å¹³ï¼
            self.grad_scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12.0)
            
            self.grad_scaler.step(self.optimizer)
            self.grad_scaler.update()
        else:
            l.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12.0)
            self.optimizer.step()

        return {'loss': l.detach().cpu().numpy()}