import torch
import torch.nn as nn
from timm.models.layers import DropPath
import math
import torchvision.ops
# ğŸ‘‡ğŸ‘‡ğŸ‘‡ è¡¥ä¸Šäº†è¿™ä¸€è¡Œï¼æ²¡æœ‰å®ƒä¼šæŠ¥é”™ï¼ ğŸ‘‡ğŸ‘‡ğŸ‘‡
from torch.utils.checkpoint import checkpoint 

# ==========================================
# ä¿®å¤åçš„å¯¼å…¥é€»è¾‘ï¼šä¸å†éšè—æŠ¥é”™
# ==========================================
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ (ç”¨äº nnU-Net è®­ç»ƒæ—¶)
    from .H_vmunet import H_SS2D
except ImportError as e:
    try:
        # å°è¯•ç»å¯¹å¯¼å…¥ (ç”¨äºå•ç‹¬ python test_model.py æµ‹è¯•æ—¶)
        from H_vmunet import H_SS2D
    except ImportError as e2:
        # å¦‚æœä¸¤æ¬¡éƒ½å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯å¹¶æŠ›å‡ºå¼‚å¸¸
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•å¯¼å…¥ H_vmunet.H_SS2D")
        print(f"   ç›¸å¯¹å¯¼å…¥é”™è¯¯: {e}")
        print(f"   ç»å¯¹å¯¼å…¥é”™è¯¯: {e2}")
        raise e2 

class DCNv2_PyTorch(nn.Module):
    """
    ã€é›¶ç¼–è¯‘ç‰ˆã€‘ä½¿ç”¨ PyTorch åŸç”Ÿ torchvision å®ç°çš„ DCNv2
    å®Œå…¨ä¸éœ€è¦ç¼–è¯‘ DCNv4ï¼Œå³æ’å³ç”¨
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # 1. ç”Ÿæˆåç§»é‡ (Offsets)
        self.conv_offset = nn.Conv2d(
            in_channels, 
            2 * kernel_size * kernel_size, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=padding
        )
        
        # 2. ç”Ÿæˆæ©ç  (Mask)
        self.conv_mask = nn.Conv2d(
            in_channels, 
            kernel_size * kernel_size, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=padding
        )
        self.sigmoid = nn.Sigmoid()
        
        # 3. æ ‡å‡†å·ç§¯æƒé‡
        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)
            
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)
        nn.init.constant_(self.conv_offset.weight, 0)
        nn.init.constant_(self.conv_offset.bias, 0)
        nn.init.constant_(self.conv_mask.weight, 0)
        nn.init.constant_(self.conv_mask.bias, 0)

    def forward(self, x):
        offset = self.conv_offset(x)
        mask = self.sigmoid(self.conv_mask(x))
        return torchvision.ops.deform_conv2d(
            input=x, 
            offset=offset, 
            weight=self.weight, 
            bias=self.bias, 
            stride=self.stride, 
            padding=self.padding, 
            mask=mask
        )

class SDG_Block(nn.Module):
    def __init__(self, dim, drop_path=0.):
        super().__init__()
        self.dcn = DCNv2_PyTorch(in_channels=dim, out_channels=dim, kernel_size=3, stride=1, padding=1)
        
        # åˆå§‹åŒ– H-SS2D (å·²ç»ç¡®ä¿å¯¼å…¥æˆåŠŸ)
        max_possible_order = int(math.log2(dim)) - 1
        safe_order = min(5, max(2, max_possible_order))
        self.h_ss2d = H_SS2D(dim=dim, order=safe_order, s=1.0)
        
        self.norm = nn.LayerNorm(dim)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        # 1. æŠŠè®¡ç®—é€»è¾‘å°è£…è¿›å‡½æ•° (ä¸ºäº†ä¼ ç»™ checkpoint)
        def _inner_forward(input_x):
            shortcut = input_x
            x = self.dcn(input_x)
            
            # ç»´åº¦è½¬æ¢ NCHW -> NHWC (LayerNorm éœ€è¦)
            x = x.permute(0, 2, 3, 1).contiguous()
            x = self.norm(x)
            
            # ç»´åº¦è½¬æ¢ NHWC -> NCHW (SS2D éœ€è¦)
            x = x.permute(0, 3, 1, 2).contiguous()
            x = self.h_ss2d(x)
            
            # æ®‹å·®è¿æ¥
            x = shortcut + self.drop_path(x)
            return x

        # 2. è®­ç»ƒæ—¶å¯ç”¨ checkpoint çœæ˜¾å­˜
        # åªæœ‰åœ¨è®­ç»ƒæ¨¡å¼ä¸”éœ€è¦æ¢¯åº¦æ—¶æ‰å¼€å¯ï¼ŒéªŒè¯/æµ‹è¯•æ—¶ä¸å¼€å¯
        if self.training and x.requires_grad:
            return checkpoint(_inner_forward, x, use_reentrant=False)
        else:
            return _inner_forward(x)